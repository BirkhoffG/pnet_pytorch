# privacy_hidden_representations
Privacy preserving ML for NLP tasks


This project explores at ways to prevent intermediate representations of neural networks from leaking private information contained in the input features while balancing the tradeoff with model accuracy. 

Key assumptions:
1. Private information may be explicitly present in the input features or might be heavily correlated with them.

2. The attacker has access to the representation function and thus the intermediate representations generated by the model at test time.

Many of the ideas implemented in this project comes from the following EMNLP 2018 papers:

Coavoux et al., [Privacy-preserving Neural Representations of Text] (http://aclweb.org/anthology/D18-1001)
Elazar et al., [Adversarial Removal of Demographic Attributes from Text Data] (http://aclweb.org/anthology/D18-1002)

I have written an accompanying blog post about this project. You can read it on [Medium] (https://medium.com/@piesauce/what-i-learned-from-emnlp2018-papers-part-2-4ae0f550ced8)


